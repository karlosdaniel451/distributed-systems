{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/16 16:08:16 WARN Utils: Your hostname, karlos-300E5M-300E5L resolves to a loopback address: 127.0.1.1; using 10.0.0.89 instead (on interface wlp2s0)\n",
      "23/01/16 16:08:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/16 16:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Six Spark Exercises to Rule Them All') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[product_id: int, product_name: string, price: int]\n",
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|0         |product_0   |22   |\n",
      "|1         |product_1   |30   |\n",
      "|2         |product_2   |91   |\n",
      "|3         |product_3   |37   |\n",
      "|4         |product_4   |145  |\n",
      "|5         |product_5   |128  |\n",
      "|6         |product_6   |66   |\n",
      "|7         |product_7   |145  |\n",
      "|8         |product_8   |51   |\n",
      "|9         |product_9   |44   |\n",
      "|10        |product_10  |53   |\n",
      "|11        |product_11  |13   |\n",
      "|12        |product_12  |104  |\n",
      "|13        |product_13  |102  |\n",
      "|14        |product_14  |24   |\n",
      "|15        |product_15  |14   |\n",
      "|16        |product_16  |38   |\n",
      "|17        |product_17  |72   |\n",
      "|18        |product_18  |16   |\n",
      "|19        |product_19  |46   |\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.read.parquet('./data/products_parquet/') \\\n",
    "    .withColumn('product_id', F.col('product_id').cast('int')) \\\n",
    "    .withColumn('price', F.col('price').cast('INT'))\n",
    "\n",
    "products.createOrReplaceTempView('products')\n",
    "\n",
    "print(products)\n",
    "products.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[order_id: int, product_id: int, seller_id: int, date: date, num_pieces_sold: int, bill_raw_text: string]\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|order_id|product_id|seller_id|      date|num_pieces_sold|       bill_raw_text|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "|       1|         0|        0|2020-07-10|             26|kyeibuumwlyhuwksx...|\n",
      "|       2|         0|        0|2020-07-08|             13|jfyuoyfkeyqkckwbu...|\n",
      "|       3|         0|        0|2020-07-05|             38|uyjihlzhzcswxcccx...|\n",
      "|       4|         0|        0|2020-07-05|             56|umnxvoqbdzpbwjqmz...|\n",
      "|       5|         0|        0|2020-07-05|             11|zmqexmaawmvdpqhih...|\n",
      "|       6|         0|        0|2020-07-01|             82|lmuhhkpyuoyslwmvX...|\n",
      "|       7|         0|        0|2020-07-04|             15|zoqweontumefxbgvu...|\n",
      "|       8|         0|        0|2020-07-08|             79|sgldfgtcxufasnvsc...|\n",
      "|       9|         0|        0|2020-07-10|             25|jnykelwjjebgkwgmu...|\n",
      "|      10|         0|        0|2020-07-08|              8|yywjfihneygcvfnyl...|\n",
      "|      11|         0|        0|2020-07-01|             10|nxwejyoeznltdhcam...|\n",
      "|      12|         0|        0|2020-07-06|             45|efmymeftivwsfljzt...|\n",
      "|      13|         0|        0|2020-07-10|             63|nxhvtospPhfnkavdy...|\n",
      "|      14|         0|        0|2020-07-03|             22|ypyusdsjzfpfbucnn...|\n",
      "|      15|         0|        0|2020-07-09|             75|ymjvbhaxffyjcwzyn...|\n",
      "|      16|         0|        0|2020-07-10|             83|phbcykkhvqsbkipwa...|\n",
      "|      17|         0|        0|2020-07-04|             54|qgnGqqnjmbqZytoug...|\n",
      "|      18|         0|        0|2020-07-04|             58|ozmllbabrnhebWcex...|\n",
      "|      19|         0|        0|2020-07-07|             33|kbrvXuzgiuinodtkg...|\n",
      "|      20|         0|        0|2020-07-09|             73|jnqjzaigjtqlfwpug...|\n",
      "+--------+----------+---------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = spark.read.parquet('./data/sales_parquet') \\\n",
    "    .withColumn('order_id', F.col('order_id').cast('int')) \\\n",
    "    .withColumn('product_id', F.col('product_id').cast('int')) \\\n",
    "    .withColumn('seller_id', F.col('seller_id').cast('int')) \\\n",
    "    .withColumn('date', F.col('date').cast('date')) \\\n",
    "    .withColumn('num_pieces_sold', F.col('num_pieces_sold').cast('int')) \\\n",
    "\n",
    "sales.createOrReplaceTempView('sales')\n",
    "\n",
    "print(sales)\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[seller_id: int, seller_name: string, daily_target: int]\n",
      "+---------+-----------+------------+\n",
      "|seller_id|seller_name|daily_target|\n",
      "+---------+-----------+------------+\n",
      "|        0|   seller_0|     2500000|\n",
      "|        1|   seller_1|      257237|\n",
      "|        2|   seller_2|      754188|\n",
      "|        3|   seller_3|      310462|\n",
      "|        4|   seller_4|     1532808|\n",
      "|        5|   seller_5|     1199693|\n",
      "|        6|   seller_6|     1055915|\n",
      "|        7|   seller_7|     1946998|\n",
      "|        8|   seller_8|      547320|\n",
      "|        9|   seller_9|     1318051|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sellers = spark.read.parquet('./data/sellers_parquet') \\\n",
    "    .withColumn('seller_id', F.col('seller_id').cast('int')) \\\n",
    "    .withColumn('daily_target', F.col('daily_target').cast('int'))\n",
    "\n",
    "sellers.createOrReplaceTempView('sellers')\n",
    "\n",
    "print(sellers)\n",
    "sellers.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up #1\n",
    "\n",
    "Find out how many orders, how many products and how many sellers are in the data.\n",
    "\n",
    "How many products have been sold at least once? Which is the product contained in more orders?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 75_000_000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sales: 20_000_040\n",
      "Number of sellers: 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of products: {products.count():_}')\n",
    "print(f'Number of sales: {sales.count():_}')\n",
    "print(f'Number of sellers: {sellers.count():_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 182:====================================================>  (79 + 4) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|products_sold|\n",
      "+-------------+\n",
      "|       993429|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.selectExpr('COUNT(DISTINCT product_id) AS products_sold').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 176:=====================================================> (81 + 2) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|products_sold|\n",
      "+-------------+\n",
      "|       993429|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Alternative version\n",
    "\n",
    "sales.select(F.count_distinct('product_id').alias('products_sold')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:====================================================>  (79 + 4) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|product_id|number_of_sales|\n",
      "+----------+---------------+\n",
      "|         0|       19000000|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.groupBy('product_id') \\\n",
    "    .agg(F.count('*').alias('number_of_sales')) \\\n",
    "    .orderBy('number_of_sales', ascending=False) \\\n",
    "    .limit(1) \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS number_of_products\n",
    "    FROM products\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS number_of_sales\n",
    "    FROM sales\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS number_of_sellers\n",
    "    FROM sellers\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:=====================================================> (81 + 2) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|products_sold|\n",
      "+-------------+\n",
      "|       993429|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(DISTINCT sales.product_id) AS products_sold\n",
    "    FROM sales\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:=====================================================> (81 + 2) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|product_id|number_of_sales|\n",
      "+----------+---------------+\n",
      "|0         |19000000       |\n",
      "+----------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT sales.product_id, COUNT(*) number_of_sales\n",
    "    FROM sales\n",
    "    GROUP BY sales.product_id\n",
    "    ORDER BY number_of_sales DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up #2\n",
    "\n",
    "How many distinct products have been sold in each day?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 239:===================================================>   (78 + 4) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------+\n",
      "|      date|number_of_distinct_products_sold|\n",
      "+----------+--------------------------------+\n",
      "|2020-07-01|                         2001370|\n",
      "|2020-07-02|                         1998985|\n",
      "|2020-07-03|                         2000651|\n",
      "|2020-07-04|                         2000949|\n",
      "|2020-07-05|                         1996618|\n",
      "|2020-07-06|                         2001302|\n",
      "|2020-07-07|                         2000807|\n",
      "|2020-07-08|                         2000451|\n",
      "|2020-07-09|                         2000402|\n",
      "|2020-07-10|                         1998505|\n",
      "+----------+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales.groupBy('date') \\\n",
    "    .agg(F.count('*').alias('number_of_distinct_products_sold')) \\\n",
    "    .orderBy('date') \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:===================================================>   (78 + 4) / 83]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------+\n",
      "|      date|number_of_distinct_products_sold|\n",
      "+----------+--------------------------------+\n",
      "|2020-07-01|                         2001370|\n",
      "|2020-07-02|                         1998985|\n",
      "|2020-07-03|                         2000651|\n",
      "|2020-07-04|                         2000949|\n",
      "|2020-07-05|                         1996618|\n",
      "|2020-07-06|                         2001302|\n",
      "|2020-07-07|                         2000807|\n",
      "|2020-07-08|                         2000451|\n",
      "|2020-07-09|                         2000402|\n",
      "|2020-07-10|                         1998505|\n",
      "+----------+--------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT sales.date, COUNT(*) as number_of_distinct_products_sold\n",
    "    FROM sales\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise #1\n",
    "\n",
    "What is the average revenue of the orders?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.select() \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d76f782f9e2bb336f39952c692d78dbf55a20fd2f5486713d2fa651d7c15ab40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
