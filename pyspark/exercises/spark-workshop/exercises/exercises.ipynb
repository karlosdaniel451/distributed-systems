{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/15 21:05:30 WARN Utils: Your hostname, karlos-300E5M-300E5L resolves to a loopback address: 127.0.1.1; using 192.168.10.18 instead (on interface enp1s0)\n",
      "23/01/15 21:05:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/15 21:05:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-workshop exercises</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f90f1d29070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('spark-workshop exercises') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Finding Most Populated Cities Per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, country: string, population: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\n",
    "    './data/spark-sql-15-input.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+----------+\n",
      "|             name|      country|population|\n",
      "+-----------------+-------------+----------+\n",
      "|           Warsaw|       Poland| 1 764 615|\n",
      "|           Cracow|       Poland|   769 498|\n",
      "|            Paris|       France| 2 206 488|\n",
      "|Villeneuve-Loubet|       France|    15 020|\n",
      "|    Pittsburgh PA|United States|   302 407|\n",
      "|       Chicago IL|United States| 2 716 000|\n",
      "|     Milwaukee WI|United States|   595 351|\n",
      "|          Vilnius|    Lithuania|   580 020|\n",
      "|        Stockholm|       Sweden|   972 647|\n",
      "|         Goteborg|       Sweden|   580 020|\n",
      "+-----------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove the whitespaces in the values of population and convert them to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, country: string, population: int]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.withColumn(\n",
    "    'population',\n",
    "    F.regexp_replace('population', r'\\s', '').cast('int')\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+----------+\n",
      "|             name|      country|population|\n",
      "+-----------------+-------------+----------+\n",
      "|           Warsaw|       Poland|   1764615|\n",
      "|           Cracow|       Poland|    769498|\n",
      "|            Paris|       France|   2206488|\n",
      "|Villeneuve-Loubet|       France|     15020|\n",
      "|    Pittsburgh PA|United States|    302407|\n",
      "|       Chicago IL|United States|   2716000|\n",
      "|     Milwaukee WI|United States|    595351|\n",
      "|          Vilnius|    Lithuania|    580020|\n",
      "|        Stockholm|       Sweden|    972647|\n",
      "|         Goteborg|       Sweden|    580020|\n",
      "+-----------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|      country|max_population|\n",
      "+-------------+--------------+\n",
      "|       Sweden|        972647|\n",
      "|       France|       2206488|\n",
      "|United States|       2716000|\n",
      "|    Lithuania|        580020|\n",
      "|       Poland|       1764615|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_population_by_country = data.groupBy('country') \\\n",
    "    .agg(F.expr('MAX(population) AS max_population'))\n",
    "\n",
    "max_population_by_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+\n",
      "|name      |population|country      |\n",
      "+----------+----------+-------------+\n",
      "|Warsaw    |1764615   |Poland       |\n",
      "|Paris     |2206488   |France       |\n",
      "|Chicago IL|2716000   |United States|\n",
      "|Vilnius   |580020    |Lithuania    |\n",
      "|Stockholm |972647    |Sweden       |\n",
      "+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.join(\n",
    "    max_population_by_country,\n",
    "    on=(data['country'] == max_population_by_country['country'])\n",
    "    & (data['population'] == max_population_by_country['max_population'])\n",
    ").drop('max_population') \\\n",
    "    .drop(max_population_by_country['country']) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------+\n",
      "|country      |name      |max_population|\n",
      "+-------------+----------+--------------+\n",
      "|United States|Chicago IL|2716000       |\n",
      "|France       |Paris     |2206488       |\n",
      "|Poland       |Warsaw    |1764615       |\n",
      "|Sweden       |Stockholm |972647        |\n",
      "|Lithuania    |Vilnius   |580020        |\n",
      "+-------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView('exercise_15_data')\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH max_population_by_country AS (\n",
    "        SELECT country, MAX(population) AS max_population\n",
    "        FROM exercise_15_data\n",
    "        GROUP BY country\n",
    "    )\n",
    "    SELECT\n",
    "        exercise_15_data.country,\n",
    "        exercise_15_data.name,\n",
    "        max_population_by_country.max_population\n",
    "    FROM exercise_15_data\n",
    "    INNER JOIN max_population_by_country\n",
    "        ON exercise_15_data.population = max_population_by_country.max_population\n",
    "            AND exercise_15_data.country = max_population_by_country.country\n",
    "    ORDER BY max_population DESC\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 18: Difference in Days Between Dates As Strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_dates(n: int, fake: Faker) -> list[datetime.date]:\n",
    "    \"\"\"Generate a list of `n` random dates between 1970/01/01 and\n",
    "    today.\n",
    "    \"\"\"\n",
    "    return [fake.date() for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1976-07-21|\n",
      "|2022-01-21|\n",
      "|1982-11-20|\n",
      "|2014-08-04|\n",
      "|1999-09-11|\n",
      "|1986-11-21|\n",
      "|1983-07-30|\n",
      "|1991-12-07|\n",
      "|2012-06-11|\n",
      "|1981-02-25|\n",
      "|1997-09-20|\n",
      "|2022-04-17|\n",
      "|1985-09-13|\n",
      "|1975-09-21|\n",
      "|1997-09-30|\n",
      "|1985-08-15|\n",
      "|1997-06-30|\n",
      "|2001-09-27|\n",
      "|2002-05-23|\n",
      "|1984-04-21|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUMBER_OF_RANDOM_DATES_TO_GENERATE = 10_000\n",
    "fake = Faker()\n",
    "\n",
    "random_dates = generate_random_dates(\n",
    "    NUMBER_OF_RANDOM_DATES_TO_GENERATE,\n",
    "    fake\n",
    ")\n",
    "\n",
    "dates = spark.createDataFrame(random_dates, schema='string') \\\n",
    "    .withColumnRenamed('value', 'date') \\\n",
    "    .withColumn('date', F.col('date').cast('date'))\n",
    "\n",
    "dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date| diff|\n",
      "+----------+-----+\n",
      "|1976-07-21|16979|\n",
      "|2022-01-21|  359|\n",
      "|1982-11-20|14666|\n",
      "|2014-08-04| 3086|\n",
      "|1999-09-11| 8527|\n",
      "|1986-11-21|13204|\n",
      "|1983-07-30|14414|\n",
      "|1991-12-07|11362|\n",
      "|2012-06-11| 3870|\n",
      "|1981-02-25|15299|\n",
      "|1997-09-20| 9248|\n",
      "|2022-04-17|  273|\n",
      "|1985-09-13|13638|\n",
      "|1975-09-21|17283|\n",
      "|1997-09-30| 9238|\n",
      "|1985-08-15|13667|\n",
      "|1997-06-30| 9330|\n",
      "|2001-09-27| 7780|\n",
      "|2002-05-23| 7542|\n",
      "|1984-04-21|14148|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates.withColumn(\n",
    "    'diff',\n",
    "    F.datediff(F.current_date(), F.col('date'))\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |    dates|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates.createOrReplaceTempView('dates')\n",
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date| diff|\n",
      "+----------+-----+\n",
      "|1976-07-21|16979|\n",
      "|2022-01-21|  359|\n",
      "|1982-11-20|14666|\n",
      "|2014-08-04| 3086|\n",
      "|1999-09-11| 8527|\n",
      "|1986-11-21|13204|\n",
      "|1983-07-30|14414|\n",
      "|1991-12-07|11362|\n",
      "|2012-06-11| 3870|\n",
      "|1981-02-25|15299|\n",
      "|1997-09-20| 9248|\n",
      "|2022-04-17|  273|\n",
      "|1985-09-13|13638|\n",
      "|1975-09-21|17283|\n",
      "|1997-09-30| 9238|\n",
      "|1985-08-15|13667|\n",
      "|1997-06-30| 9330|\n",
      "|2001-09-27| 7780|\n",
      "|2002-05-23| 7542|\n",
      "|1984-04-21|14148|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT date, DATEDIFF(CURRENT_DATE(), date) as diff\n",
    "    FROM dates\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 25: Collect values per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|group|\n",
      "+---+-----+\n",
      "|  0|    0|\n",
      "|  1|    1|\n",
      "|  2|    0|\n",
      "|  3|    1|\n",
      "|  4|    0|\n",
      "|  5|    1|\n",
      "|  6|    0|\n",
      "|  7|    1|\n",
      "|  8|    0|\n",
      "|  9|    1|\n",
      "| 10|    0|\n",
      "| 11|    1|\n",
      "| 12|    0|\n",
      "| 13|    1|\n",
      "| 14|    0|\n",
      "| 15|    1|\n",
      "| 16|    0|\n",
      "| 17|    1|\n",
      "| 18|    0|\n",
      "| 19|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_25_data = spark.range(20).withColumn('group', F.col('id') % 2)\n",
    "\n",
    "exercise_25_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+\n",
      "|group|collect_list(id)                   |\n",
      "+-----+-----------------------------------+\n",
      "|0    |[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]|\n",
      "|1    |[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]|\n",
      "+-----+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_25_data.groupBy('group') \\\n",
    "    .agg(F.collect_list('id')) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 33: Calculating Gap Between Current And Highest Salaries Per Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, name: string, department: string, salary: int]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_33_data = spark.read.csv(\n",
    "    './data/spark-sql-33-input.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "exercise_33_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+\n",
      "|id |name             |department|salary|\n",
      "+---+-----------------+----------+------+\n",
      "|1  |Hunter Fields    |IT        |15    |\n",
      "|2  |Leonard Lewis    |Support   |81    |\n",
      "|3  |Jason Dawson     |Support   |90    |\n",
      "|4  |Andre Grant      |Support   |25    |\n",
      "|5  |Earl Walton      |IT        |40    |\n",
      "|6  |Alan Hanson      |IT        |24    |\n",
      "|7  |Clyde Matthews   |Support   |31    |\n",
      "|8  |Josephine Leonard|Support   |1     |\n",
      "|9  |Owen Boone       |HR        |27    |\n",
      "|10 |Max McBride      |IT        |75    |\n",
      "+---+-----------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_33_data.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+----+\n",
      "| id|             name|department|salary|diff|\n",
      "+---+-----------------+----------+------+----+\n",
      "|  1|    Hunter Fields|        IT|    15|  60|\n",
      "|  2|    Leonard Lewis|   Support|    81|   9|\n",
      "|  3|     Jason Dawson|   Support|    90|   0|\n",
      "|  4|      Andre Grant|   Support|    25|  65|\n",
      "|  5|      Earl Walton|        IT|    40|  35|\n",
      "|  6|      Alan Hanson|        IT|    24|  51|\n",
      "|  7|   Clyde Matthews|   Support|    31|  59|\n",
      "|  8|Josephine Leonard|   Support|     1|  89|\n",
      "|  9|       Owen Boone|        HR|    27|   0|\n",
      "| 10|      Max McBride|        IT|    75|   0|\n",
      "+---+-----------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_33_data.groupBy('department') \\\n",
    "    .agg(F.max('salary').alias('max_salary')) \\\n",
    "    .join(\n",
    "        exercise_33_data,\n",
    "        on='department',\n",
    "        how='inner'\n",
    "    ).selectExpr(\n",
    "        'id',\n",
    "        'name',\n",
    "        'department',\n",
    "        'salary',\n",
    "        'max_salary - salary AS diff'\n",
    "    ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+------+----+\n",
      "|id |name             |department|salary|diff|\n",
      "+---+-----------------+----------+------+----+\n",
      "|1  |Hunter Fields    |IT        |15    |60  |\n",
      "|2  |Leonard Lewis    |Support   |81    |9   |\n",
      "|3  |Jason Dawson     |Support   |90    |0   |\n",
      "|4  |Andre Grant      |Support   |25    |65  |\n",
      "|5  |Earl Walton      |IT        |40    |35  |\n",
      "|6  |Alan Hanson      |IT        |24    |51  |\n",
      "|7  |Clyde Matthews   |Support   |31    |59  |\n",
      "|8  |Josephine Leonard|Support   |1     |89  |\n",
      "|9  |Owen Boone       |HR        |27    |0   |\n",
      "|10 |Max McBride      |IT        |75    |0   |\n",
      "+---+-----------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_33_data.createOrReplaceTempView('exercise_33_data')\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH max_salary_by_department AS (\n",
    "        SELECT department, MAX(salary) AS max_salary\n",
    "        FROM exercise_33_data\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT\n",
    "        exercise_33_data.id,\n",
    "        exercise_33_data.name,\n",
    "        exercise_33_data.department,\n",
    "        exercise_33_data.salary,\n",
    "        max_salary_by_department.max_salary - exercise_33_data.salary AS diff\n",
    "    FROM exercise_33_data\n",
    "    INNER JOIN max_salary_by_department\n",
    "        USING (department)\n",
    "    \"\"\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL version alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses:\nAggregate [(max(salary#1040) - outer(salary#904)) AS (max(salary) - outer(exercise_33_data.salary))#1036]\n+- Filter (outer(department#903) = department#1039)\n   +- SubqueryAlias exercise_33_data_inner\n      +- SubqueryAlias exercise_33_data\n         +- View (`exercise_33_data`, [id#1037,name#1038,department#1039,salary#1040])\n            +- Relation [id#1037,name#1038,department#1039,salary#1040] csv\n;\nProject [id#901, name#902, department#903, salary#904, scalar-subquery#1034 [salary#904 && department#903] AS scalarsubquery(salary, department)#1041]\n:  +- Aggregate [(max(salary#1040) - outer(salary#904)) AS (max(salary) - outer(exercise_33_data.salary))#1036]\n:     +- Filter (outer(department#903) = department#1039)\n:        +- SubqueryAlias exercise_33_data_inner\n:           +- SubqueryAlias exercise_33_data\n:              +- View (`exercise_33_data`, [id#1037,name#1038,department#1039,salary#1040])\n:                 +- Relation [id#1037,name#1038,department#1039,salary#1040] csv\n+- SubqueryAlias exercise_33_data\n   +- View (`exercise_33_data`, [id#901,name#902,department#903,salary#904])\n      +- Relation [id#901,name#902,department#903,salary#904] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# So it seems that Spark SQL does not support correlated subqueries :/\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m spark\u001b[39m.\u001b[39;49msql(\n\u001b[1;32m      4\u001b[0m     \u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m    SELECT\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m        exercise_33_data.id,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m        exercise_33_data.name,\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m        exercise_33_data.department,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m        exercise_33_data.salary,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m        (SELECT MAX(exercise_33_data_inner.salary) - exercise_33_data.salary\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m         FROM exercise_33_data AS exercise_33_data_inner\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m         WHERE exercise_33_data.department = exercise_33_data_inner.department)\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    FROM exercise_33_data\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m )\u001b[39m.\u001b[39mshow(truncate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[39m=\u001b[39m formatter\u001b[39m.\u001b[39mformat(sqlQuery, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses:\nAggregate [(max(salary#1040) - outer(salary#904)) AS (max(salary) - outer(exercise_33_data.salary))#1036]\n+- Filter (outer(department#903) = department#1039)\n   +- SubqueryAlias exercise_33_data_inner\n      +- SubqueryAlias exercise_33_data\n         +- View (`exercise_33_data`, [id#1037,name#1038,department#1039,salary#1040])\n            +- Relation [id#1037,name#1038,department#1039,salary#1040] csv\n;\nProject [id#901, name#902, department#903, salary#904, scalar-subquery#1034 [salary#904 && department#903] AS scalarsubquery(salary, department)#1041]\n:  +- Aggregate [(max(salary#1040) - outer(salary#904)) AS (max(salary) - outer(exercise_33_data.salary))#1036]\n:     +- Filter (outer(department#903) = department#1039)\n:        +- SubqueryAlias exercise_33_data_inner\n:           +- SubqueryAlias exercise_33_data\n:              +- View (`exercise_33_data`, [id#1037,name#1038,department#1039,salary#1040])\n:                 +- Relation [id#1037,name#1038,department#1039,salary#1040] csv\n+- SubqueryAlias exercise_33_data\n   +- View (`exercise_33_data`, [id#901,name#902,department#903,salary#904])\n      +- Relation [id#901,name#902,department#903,salary#904] csv\n"
     ]
    }
   ],
   "source": [
    "# So it looks like that Spark SQL does not support this type of correlated subquery :/\n",
    "# Or am I doing something wrong? (TODO find out it)\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        exercise_33_data.id,\n",
    "        exercise_33_data.name,\n",
    "        exercise_33_data.department,\n",
    "        exercise_33_data.salary,\n",
    "        (SELECT MAX(exercise_33_data_inner.salary) - exercise_33_data.salary\n",
    "         FROM exercise_33_data AS exercise_33_data_inner\n",
    "         WHERE exercise_33_data.department = exercise_33_data_inner.department)\n",
    "    FROM exercise_33_data\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d76f782f9e2bb336f39952c692d78dbf55a20fd2f5486713d2fa651d7c15ab40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
